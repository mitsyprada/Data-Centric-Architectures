{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Think about the following two situations:\n",
    "a. Analyze the data of recent power usage statistics reported to a power station and\n",
    "adjust the power generate rate if necessary.\n",
    "b. A school needs to know data about their students, and query about courses, age,\n",
    "and instructors.\n",
    "Which system would you choose for each situation? If you choose DSMS, explain in\n",
    "detail how you would implement it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. For the first situation choosing a DSMS is the best option. DSMS has the possibility to handle potentially infinite and rapidly changing data streams by offering flexible processing at the same time, although there are only limited resources such as main memory. DSMS offers a flexible query processing so that the information needed can be expressed using queries. DSMS executes a continuous query that is not only performed once, but is permanently installed. Therefore, the query is continuously executed until it is explicitly uninstalled. In this way we can adjust the power generate rate if necessary.\n",
    "\n",
    "b. For the second situation the best option is to choose a DBMS because we don't need to process that data constantly in order to make changes to a system (what the queries can do in DSMS constantly running). We need a software system that enables users to define, create, maintain and control access to the database, which in this case is a student database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Explain three of the most typical strategies for CQ processing, and for each of them,\n",
    "think of scenarios where it would be more convenient to apply it.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input rate of each stream in DSMS is trackable, which amounts to knowing how many tuples will be arriving in the future time slots we can find an optimal scheduling strategy which can give the best performance by using minimized resources. Most of the times the input rate of data stream is unpredictable and higly bursty which makes it difficult to find a feasable, optimal scheduling strategy. \n",
    "\n",
    "**FIFO** (First Imput First Output):Tuples are processed in the order of their arrival. Once a tuple is scheduled, it is processed by the operators along its  OP (operator path) until it's consumed by an intermediate operator or output to the root node. Then the next oldest tuple is scheduled.\n",
    "\n",
    "**Chain Strategy**: Near optimal scheduling strategy in terms of total internal queue size. At any time, consider all tuples that are currently on the system; of these, schedule a single time unit for the tuple that lies on the segment with the steepest slope in its lowest envelope simulation. If there are multiple such tuples, select the tuple which has the earliest arrival time.\n",
    "\n",
    "The chain strategy performs better than FIFO for the total internal queue size, but it performs worse than FIFO in terms or tuple latency. The FIFO strategy does not consider inherent properties of an operator such as selectivity and processing rate. The Chain Strategy pushes the tuples from the bottom, it inherits the bursty property of the input stream. If the input streams are bursty in nature, the output of the query processing system under the chain strategy is too.  \n",
    "\n",
    "\n",
    "**Greedy**: The query optimizer performs an exhaustive search of all possible plans for executing a query. It computes a cost estimate for each possible plan and chooses the cheapest plan according to the cost algorithms.\n",
    "For queries involving large numbers of tables, the time spent enumerating the plans and associating costs with them can be unnaceptable. The query optimizer includes an alternative mechanism for generating query plans, called greedy optimization, that can greatly reduce the length of compile time. Rather than enumerate all possible query plans (including all permutations of table join order, all combinations of tables and useful secondary indexes, and all “shapes” of query plans), the “greedy” enumeration heuristic starts with small plan fragments, using them as building blocks to construct the eventual query plan. The chosen fragments are always the lowest cost at that stage of plan construction, so even though large numbers of potential plans are not considered, those that are chosen are also based on cost estimation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Mention the main characteristics of a Query Scheduler in a Data Stream Management\n",
    "System. What do you think is the biggest problem of the scheduling in comparison with a\n",
    "Data Base Management System?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Characteristics:\n",
    "\n",
    "1-The scheduler needs to provide rate synchronization within different operators.\n",
    "\n",
    "2-Time-varying arrival rates of data streams and timevarying output rates of operators.\n",
    "\n",
    "There are continuous queries in a DSMS\n",
    "Scheduling decision take into account:\n",
    "● memory allocation across operators\n",
    "● management of buffers for incoming stream\n",
    "● performance requirements of individual queries\n",
    "\n",
    "All this is not neccessary in DBMS because it has theorical unlimited memmory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Write queries in CQL (objective is to understand the queries, but the script isn’t in SQL)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Column  family  database,  which  will  have  single  tweet stored  per  row  and  the  columns  contain  information regarding each  tweet e.g. time of  the tweet,  name of  the user publishing the tweet, text in the tweet and an id which uniquely identifies the tweet*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE  TABLE  tweets  (tweetid text,  tweet-  time  text, tweet text, postedby text, PRIMARY KEY(tweetid)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This  CQL  command  will  read  the  timeline  of  user1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT * FROM user1timeline; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following CQL command creates the column family for user1.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE TABLE user1timeline (tweetnode int, tweettime text, tweetid text, PRIMARY KEY (tweetnode, tweettime)) WITH  CLUSTERING  ORDER  BY  (tweettime  DESC, tweetid ASC); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We  run  the  following  CQL commands  to carry  out  the write operation:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT  INTO  tweets  (tweetid,  tweettime,  tweet,   postedby)  VALUES  (‘t1’,    ‘23:45’,    ‘Hello  World!!!’, ‘user1’);    INSERT  INTO  user2timeline  (tweetnode,  tweet-  time, tweetid) VALUES (201708, ‘23:45’,‘t1’);    INSERT  INTO  user3timeline  (tweetnode,  tweet-  time, tweetid) VALUES (201708, ‘23:45’, ‘t1’);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Read operation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT * FROM user2timeline;   SELECT * FROM user3timeline; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Implement a query processing script in python over a Tweeter Data Stream**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I implement query processing over a Tweeter Data Stream to count the percentage of tweets with the word 'love' over 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler, Stream, StreamListener\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'd7JX1O1nI1hoSpp2oJChCiJ8q'\n",
    "consumer_secret = 'PlF3NYkOPzwYN0cFShOwxhrXFjaV0tnH53jN9bsPDePqVFUDLs'\n",
    "\n",
    "access_token=\"1265185453982040064-4tkbE7MXqGUFAXq4EhZlGYdsUunydy\"\n",
    "access_token_secret=\"DZS4AQzKrJ6ltTKg0IZinpoIKmNyaFIuGr7C5PKQ1zs4B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8278497839813894\n"
     ]
    }
   ],
   "source": [
    "class StdOutListener(StreamListener):\n",
    "    \"\"\" A listener handles tweets that are received from the stream.\n",
    "    This is a basic listener that just prints received tweets to stdout.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.t0 = time.time()#initialize time\n",
    "        self.cnt = 0#counter for tweets\n",
    "        self.cnt_love = 0\n",
    "               \n",
    "        \n",
    "    def on_data(self, data):\n",
    "        data = json.loads(data)\n",
    "        try:\n",
    "            if (data['truncated']):#if this is true, this is an extended tweet\n",
    "                if 'love' in data['extended_tweet']['full_text']:\n",
    "                    self.cnt_love += 1\n",
    "                self.cnt += 1\n",
    "            else:\n",
    "                if 'love' in data['text']:\n",
    "                    self.cnt_love += 1\n",
    "                self.cnt += 1\n",
    "               # print(data['text'])\n",
    "            \n",
    "        except: \n",
    "            return True\n",
    "        if (time.time()-self.t0>120):# 2 minutes, 120 seconds\n",
    "            prc = (self.cnt_love/self.cnt)*100\n",
    "            print(prc)\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        \n",
    "\n",
    "l = StdOutListener()\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "stream = Stream(auth, l)\n",
    "stream.filter(track=['the'])#we filter the tweets that contain 'the'. From those, we give the percentage of tweets that contain 'love'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Give the answer of the queries below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**● percentage of tweets mentioning “Yes” during 2 minutes, from all tweets, without\n",
    "restrictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4165972337943676\n"
     ]
    }
   ],
   "source": [
    "class StdOutListener(StreamListener):\n",
    "    \"\"\" A listener handles tweets that are received from the stream.\n",
    "    This is a basic listener that just prints received tweets to stdout.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.t0 = time.time()#initialize time\n",
    "        self.cnt = 0#counter for tweets\n",
    "        self.cnt_yes = 0\n",
    "               \n",
    "        \n",
    "    def on_data(self, data):\n",
    "        data = json.loads(data)\n",
    "        try:\n",
    "            if (data['truncated']):#if this is true, this is an extended tweet\n",
    "                if 'Yes' in data['extended_tweet']['full_text']:\n",
    "                    self.cnt_yes += 1\n",
    "                self.cnt += 1\n",
    "            else:\n",
    "                if 'Yes' in data['text']:\n",
    "                    self.cnt_yes += 1\n",
    "                self.cnt += 1\n",
    "               # print(data['text'])\n",
    "            \n",
    "        except: \n",
    "            return True\n",
    "        if (time.time()-self.t0>120):# 2 minutes, 120 seconds\n",
    "            prc = (self.cnt_yes/self.cnt)*100\n",
    "            print(prc)\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        \n",
    "\n",
    "l = StdOutListener()\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "stream = Stream(auth, l)\n",
    "stream.filter(track=['the'])#we filter the tweets that contain 'the'. From those, we give the percentage of tweets that contain 'Yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**● (sliding windows) Use sliding windows, and calculate the percentage of tweets that\n",
    "mention “Yes”. Use a window size of 300 tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4445418932351049\n"
     ]
    }
   ],
   "source": [
    "class StdOutListener(StreamListener):\n",
    "    \"\"\" A listener handles tweets that are received from the stream.\n",
    "    This is a basic listener that just prints received tweets to stdout.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.t0 = time.time()#initialize time\n",
    "        self.cnt = 0#counter for tweets\n",
    "        self.cnt_yes = 0#counter for tweets with the word 'yes'\n",
    "        self.list_300 = list()#sliding window of 300 elements\n",
    "               \n",
    "        \n",
    "    def on_data(self, data):\n",
    "            if (len(self.list_300)<300):\n",
    "                data = json.loads(data)\n",
    "                try:\n",
    "                    if (data['truncated']):#if this is true, this is an extended tweet\n",
    "                        self.list_300.append(data['extended_tweet']['full_text'])#append 1 element\n",
    "                    else:\n",
    "                        self.list_300.append(data['text'])#append 1 element\n",
    "                except: \n",
    "                    return True\n",
    "            elif (len(self.list_300) >= 300):#if we reach the lenght of 300, we count the word\n",
    "                for i in self.list_300:#check each element of the window\n",
    "                        if 'Yes' in i:#check in each element (tweet) for the word 'Yes'\n",
    "                            self.cnt_yes += 1\n",
    "                           # print(data['text'])\n",
    "                        self.cnt += 1\n",
    "\n",
    "                self.list_300.pop(0)#take out the oldest element of the list\n",
    "                data = json.loads(data)\n",
    "                try:\n",
    "                    if (data['truncated']):\n",
    "                        self.list_300.append(data['extended_tweet']['full_text'])#append 1 element\n",
    "                    else:\n",
    "                        self.list_300.append(data['text'])\n",
    "                except:\n",
    "                    return True\n",
    "            if (time.time()-self.t0>120):# at 2 minutes, 120 seconds, we print the percentage\n",
    "                prc = (self.cnt_yes/self.cnt)*100\n",
    "                print(prc)\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        \n",
    "\n",
    "l = StdOutListener()\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "stream = Stream(auth, l)\n",
    "stream.filter(track=['the'])#we filter the tweets that contain 'the'. From those, we give the percentage of tweets that contain 'Yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**● (batching) percentage of tweets mentioning “No” during 2 minutes. Assume that the\n",
    "counting function has a very slow rate, use buffers (length 100) for the elements and\n",
    "compute the query answer using each tweet once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9833333333333334\n"
     ]
    }
   ],
   "source": [
    "class StdOutListener(StreamListener):\n",
    "    \"\"\" A listener handles tweets that are received from the stream.\n",
    "    This is a basic listener that just prints received tweets to stdout.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.t0 = time.time()#initialize time\n",
    "        self.cnt = 0#counter for tweets\n",
    "        self.cnt_no = 0#counter for tweets with the word 'no'\n",
    "        self.list_100 = list()#batch of 100 elements\n",
    "               \n",
    "        \n",
    "    def on_data(self, data):\n",
    "            if (len(self.list_100)<100):\n",
    "                data = json.loads(data)\n",
    "                try:\n",
    "                    if (data['truncated']):#if this is true, this is an extended tweet\n",
    "                        self.list_100.append(data['extended_tweet']['full_text'])#append 1 element\n",
    "                    else:\n",
    "                        self.list_100.append(data['text'])#append 1 element\n",
    "                except: \n",
    "                    return True\n",
    "            elif (len(self.list_100) >= 100):#if we reach the lenght of 100, we count the word\n",
    "                for i in self.list_100:#check each element of the window\n",
    "                        if 'No' in i:#check in each element (tweet) for the word 'Yes'\n",
    "                            self.cnt_no += 1\n",
    "                        self.cnt += 1\n",
    "\n",
    "                self.list_100 = list() #clear buffer\n",
    "            if (time.time()-self.t0>120):# at 2 minutes, 120 seconds, we print the percentage\n",
    "                prc = (self.cnt_no/self.cnt)*100\n",
    "                print(prc)\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        \n",
    "\n",
    "l = StdOutListener()\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "stream = Stream(auth, l)\n",
    "stream.filter(track=['the'])#we filter the tweets that contain 'the'. From those, we give the percentage of tweets that contain 'No'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**● (sampling) percentage of tweets mentioning “Hi” during 2 minutes. Assume that the\n",
    "update function is slow. Update the list with a sample of the elements. (e.g., one in one\n",
    "hundred).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9433962264150944\n"
     ]
    }
   ],
   "source": [
    "class StdOutListener(StreamListener):\n",
    "    \"\"\" A listener handles tweets that are received from the stream.\n",
    "    This is a basic listener that just prints received tweets to stdout.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.t0 = time.time()#initialize time\n",
    "        self.cnt_s = 0 #counter for sampling\n",
    "        self.cnt = 0 #counter for tweets\n",
    "        self.cnt_hi = 0 #counter for tweets with the word 'Hi'\n",
    "               \n",
    "        \n",
    "    def on_data(self, data):\n",
    "        data = json.loads(data)\n",
    "        self.cnt_s += 1\n",
    "        if (self.cnt_s>=50):#when the counter reaches 50, we analyse the tweet (we sample every 50 tweets)\n",
    "            try:\n",
    "                    if 'Hi' in data['extended_tweet']['full_text']:\n",
    "                if (data['truncated']):#if this is true, this is an extended tweet\n",
    "                        self.cnt_hi += 1\n",
    "                    self.cnt += 1\n",
    "                else:\n",
    "                    if 'Hi' in data['text']:\n",
    "                        self.cnt_hi += 1\n",
    "                    self.cnt += 1\n",
    "                   # print(data['text'])\n",
    "\n",
    "            except: \n",
    "                return True\n",
    "            self.cnt_s = 0\n",
    "        if (time.time()-self.t0>120):# 2 minutes, 120 seconds\n",
    "            prc = (self.cnt_hi/self.cnt)*100\n",
    "            print(prc)\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        \n",
    "\n",
    "l = StdOutListener()\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "stream = Stream(auth, l)\n",
    "stream.filter(track=['the'])##we filter the tweets that contain 'the'. From those, we give the percentage of tweets that contain 'Hi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to deal with a data stream query in case one of the operations of update and compute answer work more slow than the other. Batch processing helps when the update operation is fast but the compute answer operator is slow, and sampling works the other way around.\n",
    "\n",
    "Selecting recent data for processing queries makes the data analysis better, because it excludes old data that might be irrelevant. However, in my implementation above, the window had a size of 300 but every time the window is 'slided' we compute the search of the word 'Yes' over the previous 299 tweets that were analyzed before. I think this increasing computing time.\n",
    "\n",
    "There are plenty of possibilities for data analysis, and possibly useful information that can be extracted from twitter using a developer account and tweepy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
